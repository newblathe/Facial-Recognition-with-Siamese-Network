{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbcf87f-c01c-4531-85d6-37119a59b464",
   "metadata": {},
   "source": [
    "## 1. Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17406af8-ea49-4d6f-982c-63873e98354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"tensorflow<2.11\" opencv_python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ffe8d-97e4-4305-915a-417bf333f593",
   "metadata": {},
   "source": [
    "## 2. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40a2d2-bf29-4ae3-ba0a-63c8f4e49fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tensoeflow dependencies\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "import tensorflow as tf\n",
    "\n",
    "#for unique image name\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab3943-3c65-43d6-95fd-cc8c69b0999b",
   "metadata": {},
   "source": [
    "## 3. Setting GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fa5e0-c0cb-4e06-9a42-477d98c871f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022016cb-c5ca-4029-82a7-055b3ddf92e9",
   "metadata": {},
   "source": [
    "## 4. File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19320b-4bd2-446e-bcd3-f80068bc85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making paths for poitives, negatives and anchors\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b23fdd-f81f-476a-8140-121632807427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making directories for positves, negatives and anchors\n",
    "os.makedirs(POS_PATH, exist_ok=True)\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8e64e-58b4-466f-8e6f-0c2b61a24b42",
   "metadata": {},
   "source": [
    "## 5. Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e86fcc-6fb2-4cc6-8fb8-a6449606bbe1",
   "metadata": {},
   "source": [
    "### 5.1 Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75e7fc-b85e-4e00-a12c-a89553de8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncompressing the lfw dataset\n",
    "!tar xf lfw.tgz\n",
    "\n",
    "#move the lfw images to the negatives\n",
    "for directory in os.listdir('lfw'):\n",
    "  for file in os.listdir(os.path.join('lfw', directory)):\n",
    "    EX_Path = os.path.join('lfw', directory, file)\n",
    "    NEW_Path = os.path.join(NEG_PATH, file)\n",
    "    os.replace(EX_Path, NEW_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48ca54-f545-4596-b574-9eac11bda403",
   "metadata": {},
   "source": [
    "### 5.2 Positives and Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d653bb0-aaff-4a30-b95b-898903cf0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(1)  # Use 0 to access the default webcam\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    #Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        #Crop the face from the frame\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        #Resize the face to 250x250 pixels\n",
    "        face_resized = cv2.resize(face, (250, 250))\n",
    "\n",
    "        #Show the resized face\n",
    "        cv2.imshow(\"Image\", Image)\n",
    "\n",
    "        #Save as anchor image\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"a\"):\n",
    "            image = os.path.join(ANC_PATH, \"{}.jpg\".format(uuid.uuid1()))\n",
    "            cv2.imwrite(image, face_resized)\n",
    "\n",
    "        #Save as positive image\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"p\"):\n",
    "            image = os.path.join(POS_PATH, \"{}.jpg\".format(uuid.uuid1()))\n",
    "            cv2.imwrite(image, face_resized)\n",
    "\n",
    "    #Exit when 'e' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"e\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556d674-458b-4235-9d59-7e05a5902737",
   "metadata": {},
   "source": [
    "### 5.3 Data Augmentation (used in siamese_modelv2.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94e3ce-21cd-4137-9a97-fe6117bb4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(img):\n",
    "    data = []\n",
    "    for i in range(5):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta = 0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed = (np.random.randint(100), np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_contrast(img, lower = 0.6, upper = 1, seed = (1,3))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed = (np.random.randint(100), np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower = 0.9, upper = 1, seed = (np.random.randint(100), np.random.randint(100)))\n",
    "        data.append(img)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8b3f4-f6cd-47a1-9082-a0f4393a3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some less brighter images\n",
    "for file in os.listdir(os.path.join(ANC_PATH)):\n",
    "    img = cv2.imread(os.path.join(ANC_PATH, file))\n",
    "    img_lb = tf.image.adjust_brightness(img, -0.2)\n",
    "    cv2.imwrite(os.path.join(ANC_PATH, \"{}.jpg\".format(uuid.uuid1())), img_lb.numpy())\n",
    "#adding augmented images to anchor folder\n",
    "for file in os.listdir(os.path.join(ANC_PATH)):\n",
    "    img = cv2.imread(os.path.join(ANC_PATH, file))\n",
    "    augmented_images = augment(img)\n",
    "    for img in augmented_images:\n",
    "        cv2.imwrite(os.path.join(ANC_PATH, \"{}.jpg\".format(uuid.uuid1())), img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cdcc2-2a9e-4107-9315-9ba22e3d2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some less brighter images\n",
    "for file in os.listdir(os.path.join(POS_PATH)):\n",
    "    img = cv2.imread(os.path.join(POS_PATH, file))\n",
    "    img_lb = tf.image.adjust_brightness(img, -0.2)\n",
    "    cv2.imwrite(os.path.join(POS_PATH, \"{}.jpg\".format(uuid.uuid1())), img_lb.numpy())\n",
    "#adding augmented images to anchor folder\n",
    "for file in os.listdir(os.path.join(POS_PATH)):\n",
    "    img = cv2.imread(os.path.join(POS_PATH, file))\n",
    "    augmented_images = augment(img)\n",
    "    for img in augmented_images:\n",
    "        cv2.imwrite(os.path.join(POS_PATH, \"{}.jpg\".format(uuid.uuid1())), img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768019c-bc30-4ea2-b0a5-f016162e1ca0",
   "metadata": {},
   "source": [
    "## 6. Loading and Preprocessing the Dataset(Images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af162c-1ad8-48c4-a49f-9993c6cdb2af",
   "metadata": {},
   "source": [
    "### 6.1 Loading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fa69d-7f72-439a-8570-d58c2ff8a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = tf.data.Dataset.list_files(ANC_PATH+\"\\*.jpg\").take(2500)\n",
    "positives = tf.data.Dataset.list_files(POS_PATH+\"\\*.jpg\").take(2500)\n",
    "negatives = tf.data.Dataset.list_files(NEG_PATH+\"\\*.jpg\").take(2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6e7a1-cb0f-4c79-97bf-59ebd0c56354",
   "metadata": {},
   "source": [
    "### 6.2 Preprocessing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5397cc-954e-40be-ac64-c8fda28b8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (100, 100))\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbdd2e-4e53-417b-98e7-f4fb0cb35a64",
   "metadata": {},
   "source": [
    "### 6.3 Creating Labeled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8b2b4-99e0-45a2-8d83-250fb0dac350",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchors, positives, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchors)))))\n",
    "negatives = tf.data.Dataset.zip((anchors, negatives, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchors)))))\n",
    "dataset = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663806c2-50b5-4130-bbe6-63cb5750073d",
   "metadata": {},
   "source": [
    "### 6.4 Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aafba9-d0a4-4887-8494-1209e9817e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the labelled poitives and negatives\n",
    "def preprocess_two(input_image, validation_image, label):\n",
    "    return preprocess(input_image), preprocess(validation_image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c481db9-47c5-49e9-9d33-9583d1b4e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "dataset = dataset.map(preprocess_two)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd072f2b-97b2-4895-b360-6212a5b48b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set\n",
    "train_data = dataset.take(round(len(dataset)*0.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93fdac-1400-4060-950d-f78c0fc3f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set\n",
    "test_data = dataset.skip(round(len(dataset)*0.7))\n",
    "test_data = test_data.take(round(len(dataset)*0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001d463-4633-4c02-9958-abb9402c56f5",
   "metadata": {},
   "source": [
    "## 7. Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a8d67-3b86-47a3-9714-1210e6f493ed",
   "metadata": {},
   "source": [
    "### 7.1 Creating Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a49e3-45dc-4245-a96c-b8598508f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding():\n",
    "    #1st block\n",
    "    inp = Input(shape = (100, 100, 3), name = \"Input Layer\")\n",
    "    c1 = Conv2D(64, (10, 10), activation = \"relu\")(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding = \"same\")(c1)\n",
    "                      \n",
    "    #2nd block\n",
    "    c2 = Conv2D(128, (7,7), activation = \"relu\")(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding = \"same\")(c2)\n",
    "    \n",
    "    #3rd block\n",
    "    c3 = Conv2D(128, (4,4), activation = \"relu\")(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding = \"same\")(c3)\n",
    "    \n",
    "    #4th block\n",
    "    c4 = Conv2D(256, (4,4), activation = \"relu\")(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation = \"sigmoid\")(f1)\n",
    "\n",
    "    return Model(inputs = [inp], outputs = [d1], name = \"embedding\")                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722b5a6-b9d1-4111-b7df-42404b18a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f165eb1-92d4-4fcc-a90c-1c8b28f433a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1637a4-9354-4d44-a4d5-fdea24b307d0",
   "metadata": {},
   "source": [
    "### 7.2 Creating Siamese Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eaa77-cbee-435a-bcc7-9d5189616505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667c165-da5f-49c8-93cc-628450d99abd",
   "metadata": {},
   "source": [
    "### 7.3 Combining the above two to make the Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f82fb-8e8f-4002-a0e2-1b0ad2bd32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    input_image = Input(shape = (100, 100, 3), name = \"input_img\")\n",
    "    validation_image = Input(shape = (100, 100, 3), name = \"validation_img\")\n",
    "\n",
    "    siamese_layer = L1Dist()\n",
    "    distances = siamese_layer(embedding_model(input_image), embedding_model(validation_image))\n",
    "\n",
    "    classifier = Dense(1, activation = \"sigmoid\")(distances)\n",
    "    return Model(inputs = [input_image, validation_image], outputs = [classifier], name = \"siamese_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95df68c-db16-46cc-85e3-37a456564a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20234963-1eeb-4e01-b395-95fe2a70f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b9b47-ffe1-4576-9bf5-4f527ba2c8b9",
   "metadata": {},
   "source": [
    "## 8. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af5bcb-8774-40ca-a363-f7aeb285f133",
   "metadata": {},
   "source": [
    "### 8.1 Setting up the Loss and Optimizer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5204c-decf-4bea-9251-09eeaa792cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046063d5-e8b5-483d-8f1c-c4828353d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260dad4-f073-45af-8386-2973aecd8113",
   "metadata": {},
   "source": [
    "### 8.2 Making the Checkpoints directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610c80e-baf8-4176-b88f-2ac3f7280a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = \"./training_checkpoints\"\n",
    "checkpoints_prefix = os.path.join(checkpoints_path, \"ckpt\")\n",
    "checkpoints = tf.train.Checkpoint(optimizer = optimizer, siamese_model = siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a640378-b189-456a-8f52-e678a75c5a03",
   "metadata": {},
   "source": [
    "### 8.3 Building a Function to find and apply Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10cfee-7cc5-42c7-9cac-2bc88b79aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradients(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "        yhat = siamese_model(X, training = True)\n",
    "        loss = binary_cross_entropy(yhat, y)\n",
    "    grads = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, siamese_model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962a621-6c2e-4664-ac9c-ef5602869f34",
   "metadata": {},
   "source": [
    "### 8.4 Creating the Training loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1d4d4-997d-49e2-8d81-6395dbf3c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33894d-cb8e-4a25-9352-569c231d6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(\"\\n Epoch {}/{}\".format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "\n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = gradients(batch)\n",
    "            yhat = siamese_model.predict(batch[:2], verbose = 0)\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat) \n",
    "            progbar.update(idx+1)\n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "        if epoch%10 == 0:\n",
    "            checkpoints.save(file_prefix = checkpoints_prefix)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53bf07-e88a-4ec1-b0ac-53cffa10513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94533b45-1702-4913-b2c3-72e218a371bc",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201e8b6-e472-4ee6-a8cd-b3787d61251d",
   "metadata": {},
   "source": [
    "### 9.1 Importing the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749666-c3bd-4b55-9754-2f7ff54ff131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd705a-199a-476f-97c4-648fdd2ae9a9",
   "metadata": {},
   "source": [
    "### 9.2 Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc1bd8-b63a-423c-9a13-ceeff9938df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "for test_input, test_validation, ytrue in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_validation], verbose = 0)\n",
    "    r.update_state(ytrue, yhat)\n",
    "    p.update_state(ytrue, yhat)\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f360a74-ec00-47dc-a659-13abc890bb4a",
   "metadata": {},
   "source": [
    "## 10. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad504ad-0564-4777-bc95-f71e96d9b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save(\"models/siamese_modelv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addcdc7-dacb-40fb-b1d2-f4c7cd1e3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"models/siamese_modelv2.h5\", custom_objects = {\"L1Dist\":L1Dist, \"BinaryCrossentrpy\":tf.keras.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302abc1-45c1-4d50-bd24-3561a3be25a0",
   "metadata": {},
   "source": [
    "## 11. Real Time Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca6ef6-4ced-44aa-8e93-4d0481806f5f",
   "metadata": {},
   "source": [
    "### 11.1 Verification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffab656-aa78-4f9f-ab0a-ee67bc3d2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification(model, detection_threshold, verification_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join(\"application_data\", \"verification_images\")):\n",
    "        input_image = preprocess(os.path.join(\"application_data\", \"input_image\", \"input_image.jpg\"))\n",
    "        verification_image = preprocess(os.path.join(\"application_data\", \"verification_images\", image))\n",
    "\n",
    "        #prediction for the results\n",
    "        result = model.predict(list(np.expand_dims([input_image, verification_image], axis = 1)), verbose = 0)\n",
    "        results.append(result)\n",
    "\n",
    "    #verifying\n",
    "    detection = np.sum(np.array(results)>detection_threshold)\n",
    "    verification = detection/len(os.listdir(os.path.join(\"application_data\", \"verification_images\")))\n",
    "    verified = verification>verification_threshold\n",
    "\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269727ec-a254-4ce3-973b-4e2961ecbf1a",
   "metadata": {},
   "source": [
    "### 11.2 Webcam Capture and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489d4f7-a5b9-4404-b589-bc2b8abe5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    #Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        #Crop the face from the frame\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        #Resize the face to 250x250 pixels\n",
    "        face_resized = cv2.resize(face, (250, 250))\n",
    "\n",
    "        #Show the resized face\n",
    "        cv2.imshow(\"Verification\", Image)\n",
    "\n",
    "        #Fill up the verification_images folder with images to test the input_image against\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"v\"):\n",
    "            cv2.imwrite(os.path.join(\"application_data\", \"verification_images\", \"{}.jpg\".format(uuid.uuid1())), face_resized)\n",
    "\n",
    "        #Save the image to input_image\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"t\"):\n",
    "            cv2.imwrite(os.path.join(\"application_data\", \"input_image\", \"input_image.jpg\"), face_resized)\n",
    "            results, verified = verification(model, 0.855, 0.6)\n",
    "            if verified == True:\n",
    "                print(\"You are verified\")\n",
    "            else:\n",
    "                print(\"Unverified\")\n",
    "\n",
    "    #Exit when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
